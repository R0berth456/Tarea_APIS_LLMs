{
  "cells": [
    {
      "metadata": {
        "id": "dcd16efe7639ac05"
      },
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 9: Uso de la API de Google Gemini\n",
        "\n",
        "En este ejercicio vamos a aprender a utilizar la API de OpenAI\n",
        "\n",
        "## 1. Uso básico\n",
        "\n",
        "El siguiente código sirve para conectarse con la API de Google Gemini de forma básica"
      ],
      "id": "dcd16efe7639ac05"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "I4Vc274NWg6q"
      },
      "id": "I4Vc274NWg6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"API_KEY\")"
      ],
      "metadata": {
        "id": "P8jwu57IWkQz"
      },
      "id": "P8jwu57IWkQz",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-02T15:42:17.205511Z",
          "start_time": "2025-07-02T15:42:11.198891Z"
        },
        "id": "e1a6608515f3af8d"
      },
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=\"AIzaSyCoHGiv93eV3ItTHWAe_tfLFxFMqEP25xo\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    contents=\"Explain how AI works in a few words\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "id": "e1a6608515f3af8d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3a30de61bd443d1d"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Retrieval"
      ],
      "id": "3a30de61bd443d1d"
    },
    {
      "metadata": {
        "id": "5488be80d85df276"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Cargo el corpus de 20 News Groups"
      ],
      "id": "5488be80d85df276"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-30T15:14:54.799009Z",
          "start_time": "2025-06-30T15:14:47.902617Z"
        },
        "id": "36aa2a96f75e4a43"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroupsdocs = newsgroups.data"
      ],
      "id": "36aa2a96f75e4a43",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "type(newsgroupsdocs), len(newsgroupsdocs)"
      ],
      "metadata": {
        "id": "bgzmvOo3bLa5"
      },
      "id": "bgzmvOo3bLa5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(newsgroupsdocs, columns=[\"text\"])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "sEWTiLHkbS-R"
      },
      "id": "sEWTiLHkbS-R",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16237cc6ae2f7853"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Transformo a embeddings"
      ],
      "id": "16237cc6ae2f7853"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "\n",
        "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
        "\n",
        "# Limpieza básica\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "df[\"text_norm\"] = df[\"text\"].astype(str).map(normalize_text)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "df8TxB--bCEJ"
      },
      "id": "df8TxB--bCEJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text: str, max_chars: int = 800, overlap: int = 100):\n",
        "    \"\"\"\n",
        "    Chunking por caracteres.\n",
        "    max_chars ~ 600-1000 suele funcionar bien.\n",
        "    overlap ayuda a no cortar ideas a la mitad.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    n = len(text)\n",
        "    while start < n:\n",
        "        end = min(start + max_chars, n)\n",
        "        chunk = text[start:end]\n",
        "        chunk = chunk.strip()\n",
        "        if len(chunk) > 0:\n",
        "            chunks.append(chunk)\n",
        "        if end == n:\n",
        "            break\n",
        "        start = max(0, end - overlap)\n",
        "    return chunks\n",
        "\n",
        "records = []\n",
        "for i, row in df.iterrows():\n",
        "    chunks = chunk_text(row[\"text_norm\"], max_chars=800, overlap=100)\n",
        "    for j, ch in enumerate(chunks):\n",
        "        records.append({\n",
        "            \"doc_id\": int(i),\n",
        "            \"chunk_id\": j,\n",
        "            \"text\": ch\n",
        "        })\n",
        "\n",
        "chunks_df = pd.DataFrame(records)\n",
        "chunks_df.head(), len(chunks_df)"
      ],
      "metadata": {
        "id": "6kZiUliHbCls"
      },
      "id": "6kZiUliHbCls",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f8b271aa4b20ace0"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MODEL_NAME = \"intfloat/e5-base-v2\"   # recomendado para retrieval\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "# Textos a indexar (pasajes)\n",
        "passages = [\"passage: \" + t for t in chunks_df[\"text\"].tolist()]"
      ],
      "id": "f8b271aa4b20ace0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings (N x D)\n",
        "# Se debe usar normalize_embeddings=True para similitud coseno\n",
        "embeddings = model.encode(\n",
        "    passages,\n",
        "    batch_size=16,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        ").astype(\"float32\")"
      ],
      "metadata": {
        "id": "6FDlNNN-cwDm"
      },
      "id": "6FDlNNN-cwDm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings.shape, embeddings.dtype)"
      ],
      "metadata": {
        "id": "T999qOX7iqVX"
      },
      "id": "T999qOX7iqVX",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "939af4d8947ba81"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3 Creo una query y hago la búsqueda"
      ],
      "id": "939af4d8947ba81"
    },
    {
      "metadata": {
        "id": "9da75a5ce3c09aec"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "def embed_query(query: str) -> np.ndarray:\n",
        "    q = \"query: \" + query\n",
        "    vec = model.encode(\n",
        "        [q],\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    ).astype(\"float32\")\n",
        "    return vec\n",
        "\n",
        "query_text = \"Battery measuring\"\n",
        "\n",
        "query_vec = embed_query(query_text)\n",
        "query_vec.shape"
      ],
      "id": "9da75a5ce3c09aec"
    },
    {
      "metadata": {
        "id": "d6b0a8f32632e705"
      },
      "cell_type": "markdown",
      "source": [
        "Obtengo los 5 documentos más similares a mi query"
      ],
      "id": "d6b0a8f32632e705"
    },
    {
      "metadata": {
        "id": "e0f02ff1b75a2864"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "!pip install faiss-cpu\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Dimension de los embeddings\n",
        "D = embeddings.shape[1]\n",
        "\n",
        "# Creamos un índice FAISS Flat Inner Product (IP) ya que los embeddings están normalizados\n",
        "index = faiss.IndexFlatIP(D)\n",
        "\n",
        "# Añadimos los embeddings al índice\n",
        "index.add(embeddings)\n",
        "\n",
        "# Número de documentos a recuperar\n",
        "k = 5\n",
        "\n",
        "# Realizamos la búsqueda\n",
        "distances, indices = index.search(query_vec, k)\n",
        "\n",
        "print(f\"Top {k} documentos más relevantes para la consulta '{query_text}':\")\n",
        "for i in range(k):\n",
        "    doc_index = indices[0][i]\n",
        "    score = distances[0][i]\n",
        "    print(f\"\\n--- Documento {i+1} (Score: {score:.4f}) ---\")\n",
        "    print(f\"{passages[doc_index]}\")"
      ],
      "id": "e0f02ff1b75a2864"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uso del LLM (Gemini) con los resultados anteriores"
      ],
      "metadata": {
        "id": "ptvsBo5klYeY"
      },
      "id": "ptvsBo5klYeY"
    },
    {
      "cell_type": "code",
      "source": [
        "context_docs = []\n",
        "for i in range(k):\n",
        "    doc_index = indices[0][i]\n",
        "    context_docs.append(passages[doc_index])\n",
        "\n",
        "context = \"\\n\\n\".join(context_docs)\n",
        "\n",
        "prompt = f\"Based on the following documents and the query '{query_text}', provide a concise summary.\\n\\nDocuments:\\n{context}\"\n",
        "\n",
        "response_gemini = client.models.generate_content(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(\"Summary from Gemini:\")\n",
        "print(response_gemini.text)"
      ],
      "metadata": {
        "id": "rSmunRHLlckO"
      },
      "id": "rSmunRHLlckO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}